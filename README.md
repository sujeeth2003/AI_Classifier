# AI_Classifier
AI Image Generator Fingerprinting

This repository contains a research prototype for identifying which AI image generation model produced a given image.
The approach focuses on interpretable, low-level image statistics rather than deep end-to-end neural networks.

The goal is to study whether popular diffusion models leave distinct, measurable fingerprints in their generated images.

Problem Statement

With the rapid adoption of generative image models, it is increasingly important to:

Attribute images to their source models

Study model-specific artifacts and biases

Support research in trustworthy and interpretable AI

This project explores whether simple statistical image features are sufficient to distinguish images generated by different diffusion models.

Models Considered

The current implementation includes images generated using:

Stable Diffusion v1.5

Stable Diffusion XL

PixArt (Transformer-based diffusion)

Each model generates images from the same set of prompts to reduce semantic bias.

Methodology

The pipeline consists of three stages:

1. Dataset Generation

Images are generated locally in Google Colab using open-source diffusion models

Same prompts, resolution, and sampling strategy across models

Images are organized by model label

2. Feature Extraction

For each image, the following handcrafted features are extracted:

Color statistics

Mean and standard deviation of RGB channels

RGB channel contribution percentages

Mean HSV values

Texture and sharpness

Laplacian variance (sharpness)

Sobel edge magnitude

Frequency domain

FFT energy (high-frequency content)

Structural properties

Entropy

Contrast

Brightness

All extracted features are stored in a CSV file for reproducibility and analysis.

3. Classification

Classical machine learning models (Random Forest, Logistic Regression)

Train/test split with stratification

Evaluation using precision, recall, F1-score, and confusion matrix

Results

Preliminary experiments with three models and limited samples show:

Clear separability between generators

Strong performance using only handcrafted features

Stable Diffusion XL exhibits particularly distinct statistical signatures

These results suggest that diffusion models imprint detectable low-level patterns independent of image content.

Repository Structure
AI_Classifier/
│
├── data_generation/
│   └── generate_images.py
│
├── feature_extraction/
│   └── extract_features.py
│
├── classification/
│   └── train_classifier.py
│
├── image_features.csv
├── requirements.txt
└── README.md

How to Run

Generate images using the provided diffusion pipelines

Run feature extraction to create the CSV file

Train and evaluate classifiers using the extracted features

All experiments were conducted in Google Colab with GPU support.

Future Work

Scaling the dataset to hundreds of images per model

Adding advanced texture descriptors (GLCM, wavelets)

Comparing handcrafted features with CLIP embeddings

Evaluating generalization to unseen prompts and models

Exploring robustness against post-processing and compression

Motivation

This project is intended as a foundation for research in:

AI model attribution

Trustworthy and interpretable machine learning

Forensic analysis of generative models

It is designed to be lightweight, explainable, and reproducible.

Author

Sujeeth Sukumar
MS in Data Science
University of Maryland, College Park
Email: sujeeth@umd.edu
